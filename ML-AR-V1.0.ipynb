{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from configparser import ConfigParser\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#########################################################\n",
    "##                                                     ##\n",
    "##              IMPORTANT                              ##\n",
    "## If it's the first time you run the program, please  ##\n",
    "## uncomment the following lines :                     ##\n",
    "##                                                     ##\n",
    "#########################################################\n",
    "\n",
    "#nltk.dowload('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load reviews and aggregate them in a str var\n",
    "### Arguments : \n",
    "#   - path : The path of the file containing our data\n",
    "### Output : \n",
    "#   - df : the dataframe corresponding to our data file\n",
    "#   - reviews : An array containing all the reviews\n",
    "def retrieveData(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "\n",
    "    reviews = []\n",
    "    for review in df['reviewText'].items():\n",
    "        reviews.append(review[1])\n",
    "\n",
    "    return (df, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and return ('word',#apparitions) for the most common nb_token\n",
    "def tokenize(reviews, nb_tokens=100):\n",
    "    #We start with a string containg all the reviews\n",
    "    #We retrieve all the words, then only keep alphabetic ones (no numbers)\n",
    "    tokens = nltk.tokenize.word_tokenize(reviews)\n",
    "    alpha_tokens = [t for t in tokens if t.isalpha()] \n",
    "    #We get rid of capital letters, in order to count word occurence properly\n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "\n",
    "    #We get rid of stop words \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    no_stops_tokens = [t for t in lower_tokens if t not in(stop_words)]\n",
    "\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(t) for t in no_stops_tokens]\n",
    "    count = Counter(lemmatized_tokens)\n",
    "    \n",
    "    return count.most_common(nb_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag filter\n",
    "def tag(tokens, tag):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return [t[0] for t in tagged if t[1] == tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract most relevant features from all the reviews\n",
    "def extract(reviews):\n",
    "    totalReviews = \" \"\n",
    "    for review in reviews:\n",
    "        totalReviews = totalReviews + review + \"\\n\"\n",
    "        \n",
    "    # Retrieve the 300 most common relevant words \n",
    "    token_count = tokenize(totalReviews, 300)\n",
    "    tokens = [t[0] for t in token_count]\n",
    "        \n",
    "    # Retrieve the nouns out of the 300 most common words\n",
    "    nouns = tag(tokens, 'NN')\n",
    "    print('nouns : ', nouns[0:30])\n",
    "    #adjectives=tag(tokens,'JJ')\n",
    "    #return nouns[0:30], adjectives[0:20]\n",
    "    return nouns[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering most common words if reviews\n",
    "def filter(review, features):\n",
    "    tokens = [t[0] for t in tokenize(review)]\n",
    "    filtered_tokens = [t for t in tokens if t in features]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(df, reviews):\n",
    "    # We call the extract method to retrieve the most relevant words (features)\n",
    "    features = extract(reviews)  \n",
    "    \n",
    "    # For each review, we only keep the words that are features\n",
    "    filtered_tokens = [filter(review, features) for review in reviews]\n",
    "    filtered_reviews = []\n",
    "    for f in filtered_tokens:\n",
    "        review = \"\"\n",
    "        for t in f:\n",
    "            review = review + t + \" \"\n",
    "        filtered_reviews.append(review)\n",
    "\n",
    "    # We create a column for each feature\n",
    "    # If that feature is mentioned in the review : the value is 1, else 0\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    x = cv.fit_transform(filtered_reviews)\n",
    "    return pd.DataFrame(x.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns :  ['case', 'phone', 'otterbox', 'iphone', 'screen', 'protection', 'time', 'product', 'get', 'defender', 'rubber', 'color', 'bulky', 'protector', 'use', 'drop', 'look', 'price', 'work', 'box', 'feel', 'thing', 'month', 'year', 'silicone', 'clip', 'pocket', 'belt', 'part', 'quality']\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read('init.cfg')\n",
    "path = config['RESOURCES']['path']\n",
    "dataFile = config['RESOURCES']['dataFile']\n",
    "\n",
    "# We retrieve a dataframe corresponding to our data file\n",
    "# and an array containing only the reviews\n",
    "df, reviews = retrieveData(path + dataFile)\n",
    "\n",
    "# We add to it columns. One for each feature (relevant word).\n",
    "# If a review contains a feature, the corresponding column will have value 1, else 0.\n",
    "df_features = prepareData(df, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(837, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "belt          0\n",
       "box           0\n",
       "bulky         0\n",
       "case          0\n",
       "clip          0\n",
       "color         0\n",
       "defender      0\n",
       "drop          0\n",
       "feel          0\n",
       "get           0\n",
       "iphone        1\n",
       "look          0\n",
       "month         0\n",
       "otterbox      0\n",
       "part          0\n",
       "phone         0\n",
       "pocket        0\n",
       "price         1\n",
       "product       1\n",
       "protection    0\n",
       "protector     0\n",
       "quality       0\n",
       "rubber        0\n",
       "screen        0\n",
       "silicone      0\n",
       "thing         0\n",
       "time          0\n",
       "use           0\n",
       "work          0\n",
       "year          0\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
